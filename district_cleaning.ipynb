{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca86be99",
   "metadata": {},
   "source": [
    "# Aadhaar Enrolment Dataset: District Name Cleaning\n",
    "## Cleaning duplicate and inconsistent district names in Indian Aadhaar enrolment data\n",
    "\n",
    "This notebook standardizes district names while respecting state-district pairs and identifies potential duplicates for manual review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c99e0208",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Import Required Libraries\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d197bd9e",
   "metadata": {},
   "source": [
    "## Section 1: Load and Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fb6fa98",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m df_list = []\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m csv_files:\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     file_path = \u001b[43mos\u001b[49m.path.join(data_dir, file)\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(file_path):\n\u001b[32m     14\u001b[39m         df_list.append(pd.read_csv(file_path))\n",
      "\u001b[31mNameError\u001b[39m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Load all CSV files and concatenate them into a single dataframe\n",
    "data_dir = r'e:\\MY project\\adhar first project'\n",
    "csv_files = [\n",
    "    'api_data_aadhar_enrolment_0_500000.csv',\n",
    "    'api_data_aadhar_enrolment_500000_1000000.csv',\n",
    "    'api_data_aadhar_enrolment_1000000_1006029.csv'\n",
    "]\n",
    "\n",
    "# Load and combine all CSV files\n",
    "df_list = []\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    if os.path.exists(file_path):\n",
    "        df_list.append(pd.read_csv(file_path))\n",
    "        print(f\"Loaded: {file}\")\n",
    "\n",
    "# Concatenate all dataframes\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "print(f\"\\nTotal records: {len(df)}\")\n",
    "\n",
    "# Display basic information\n",
    "print(\"\\n=== Dataset Shape and Info ===\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nData Types:\\n{df.dtypes}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head(10))\n",
    "print(f\"\\nDataset Info:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd3a263a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Initial District Data Issues ===\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Check for initial district issues\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Initial District Data Issues ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal unique districts: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdf\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mdistrict\u001b[39m\u001b[33m'\u001b[39m].nunique()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing districts (NaN): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf[\u001b[33m'\u001b[39m\u001b[33mdistrict\u001b[39m\u001b[33m'\u001b[39m].isna().sum()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSample of unique district values:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Check for initial district issues\n",
    "print(\"\\n=== Initial District Data Issues ===\")\n",
    "print(f\"Total unique districts: {df['district'].nunique()}\")\n",
    "print(f\"Missing districts (NaN): {df['district'].isna().sum()}\")\n",
    "print(f\"\\nSample of unique district values:\")\n",
    "print(df['district'].unique()[:20])\n",
    "\n",
    "# Check for districts with leading/trailing spaces or multiple spaces\n",
    "print(f\"\\nDistricts with leading/trailing spaces:\")\n",
    "spaced_districts = df[df['district'].str.strip() != df['district']]['district'].unique()\n",
    "print(f\"Count: {len(spaced_districts)}\")\n",
    "if len(spaced_districts) > 0:\n",
    "    print(spaced_districts[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531e19f8",
   "metadata": {},
   "source": [
    "## Section 2: Replace Invalid District Values\n",
    "\n",
    "Replace invalid values such as 'Nan', 'None', 'Unknown', '0' with actual NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174d65a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define invalid district values that should be replaced with NaN\n",
    "# Using case-insensitive matching\n",
    "invalid_values = ['nan', 'none', 'unknown', '0', 'na', 'n/a', '-', '']\n",
    "\n",
    "# Create a copy of the dataframe for processing\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# Replace invalid values (case-insensitive string matching)\n",
    "mask = df_cleaned['district'].astype(str).str.lower().isin(invalid_values)\n",
    "print(f\"Records with invalid district values: {mask.sum()}\")\n",
    "if mask.sum() > 0:\n",
    "    print(\"Invalid values found and being replaced with NaN:\")\n",
    "    print(df_cleaned[mask]['district'].value_counts())\n",
    "\n",
    "# Replace invalid values with NaN\n",
    "df_cleaned.loc[mask, 'district'] = np.nan\n",
    "\n",
    "print(f\"\\nMissing districts after replacement: {df_cleaned['district'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f9e1b7",
   "metadata": {},
   "source": [
    "## Section 3: Standardize District Names\n",
    "\n",
    "Apply text transformations: strip spaces, remove extra internal spaces, and convert to title case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15638da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to standardize district names\n",
    "def standardize_district_name(name):\n",
    "    \"\"\"\n",
    "    Standardize a district name by:\n",
    "    1. Stripping leading/trailing whitespace\n",
    "    2. Removing extra internal spaces (multiple spaces -> single space)\n",
    "    3. Converting to title case\n",
    "    \"\"\"\n",
    "    if pd.isna(name):\n",
    "        return name\n",
    "    \n",
    "    # Convert to string if not already\n",
    "    name = str(name)\n",
    "    \n",
    "    # Strip leading/trailing spaces\n",
    "    name = name.strip()\n",
    "    \n",
    "    # Remove extra internal spaces (replace multiple spaces with single space)\n",
    "    name = ' '.join(name.split())\n",
    "    \n",
    "    # Convert to title case\n",
    "    name = name.title()\n",
    "    \n",
    "    return name\n",
    "\n",
    "# Apply standardization to all district names\n",
    "print(\"Applying standardization to district names...\")\n",
    "df_cleaned['district'] = df_cleaned['district'].apply(standardize_district_name)\n",
    "\n",
    "print(\"\\n=== After Standardization ===\")\n",
    "print(f\"Total unique districts: {df_cleaned['district'].nunique()}\")\n",
    "print(f\"Missing districts: {df_cleaned['district'].isna().sum()}\")\n",
    "print(f\"\\nSample of standardized district values:\")\n",
    "print(df_cleaned['district'].unique()[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68ad531",
   "metadata": {},
   "source": [
    "## Section 4: Apply Manual District Mapping\n",
    "\n",
    "Handle well-known district name variations using a carefully maintained mapping dictionary.\n",
    "Only apply mappings for established district renames, not for speculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363e685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual mapping dictionary for well-known district name changes\n",
    "# Only includes official/common renames recognized across India\n",
    "# Note: Apply mappings per state where applicable\n",
    "\n",
    "district_mapping = {\n",
    "    # Karnataka\n",
    "    ('Karnataka', 'Bangalore'): 'Bengaluru',\n",
    "    ('Karnataka', 'Bangalore Urban'): 'Bengaluru Urban',\n",
    "    ('Karnataka', 'Bangalore Rural'): 'Bengaluru Rural',\n",
    "    \n",
    "    # West Bengal\n",
    "    ('West Bengal', 'Calcutta'): 'Kolkata',\n",
    "    ('West Bengal', '24 Parganas (North)'): 'North Twenty Four Parganas',\n",
    "    ('West Bengal', '24 Parganas (South)'): 'South Twenty Four Parganas',\n",
    "    \n",
    "    # Telangana / Andhra Pradesh\n",
    "    ('Telangana', 'Hyderabad'): 'Hyderabad',\n",
    "    ('Andhra Pradesh', 'Hyderabad'): 'Ranga Reddy',\n",
    "    \n",
    "    # Bihar\n",
    "    ('Bihar', 'Darbhanga'): 'Darbhanga',\n",
    "    \n",
    "    # Maharashtra - Ahmednagar to Ahmadnagar variants\n",
    "    ('Maharashtra', 'Ahmednagar'): 'Ahmednagar',\n",
    "    \n",
    "    # Uttar Pradesh - Allahabad to Prayagraj\n",
    "    ('Uttar Pradesh', 'Allahabad'): 'Prayagraj',\n",
    "}\n",
    "\n",
    "# Apply the manual mapping\n",
    "print(\"=== Applying Manual District Mapping ===\")\n",
    "mapping_applied_count = 0\n",
    "\n",
    "for (state, old_district), new_district in district_mapping.items():\n",
    "    # Find rows matching state and standardized old district name\n",
    "    mask = (df_cleaned['state'] == state) & (df_cleaned['district'] == old_district)\n",
    "    count = mask.sum()\n",
    "    \n",
    "    if count > 0:\n",
    "        df_cleaned.loc[mask, 'district'] = new_district\n",
    "        mapping_applied_count += count\n",
    "        print(f\"Mapped: ({state}, {old_district}) -> {new_district} [{count} records]\")\n",
    "\n",
    "print(f\"\\nTotal records updated via manual mapping: {mapping_applied_count}\")\n",
    "print(f\"Total unique districts after mapping: {df_cleaned['district'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d54aa44",
   "metadata": {},
   "source": [
    "## Section 5: Identify Potential Duplicate Districts by State\n",
    "\n",
    "Group by (state, district) to identify potential duplicates. \n",
    "This allows manual review without automatically dropping rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6aa0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by (state, district) to identify duplicates\n",
    "# Important: We're NOT automatically merging, just identifying for manual review\n",
    "\n",
    "state_district_counts = df_cleaned.groupby(['state', 'district']).size().reset_index(name='record_count')\n",
    "state_district_counts = state_district_counts.sort_values(['state', 'record_count'], ascending=[True, False])\n",
    "\n",
    "print(\"=== State-District Pair Summary ===\")\n",
    "print(f\"Total unique (state, district) pairs: {len(state_district_counts)}\")\n",
    "print(f\"States in dataset: {df_cleaned['state'].nunique()}\")\n",
    "\n",
    "# Identify potential duplicates: Similar names within same state\n",
    "# These could be real variations (Urban/Rural) or actual duplicates\n",
    "print(\"\\n=== Potential Duplicate Analysis ===\")\n",
    "print(\"(Looking for similar district names within the same state)\")\n",
    "\n",
    "duplicate_candidates = []\n",
    "\n",
    "for state in df_cleaned['state'].unique():\n",
    "    if pd.isna(state):\n",
    "        continue\n",
    "    \n",
    "    state_districts = state_district_counts[state_district_counts['state'] == state]['district'].tolist()\n",
    "    \n",
    "    # Look for districts that differ only by minor variations\n",
    "    # (e.g., suffixes like Urban, Rural, District)\n",
    "    for i, dist1 in enumerate(state_districts):\n",
    "        if pd.isna(dist1):\n",
    "            continue\n",
    "        \n",
    "        base_name1 = dist1.replace(' Urban', '').replace(' Rural', '').replace(' District', '').strip()\n",
    "        \n",
    "        for dist2 in state_districts[i+1:]:\n",
    "            if pd.isna(dist2):\n",
    "                continue\n",
    "            \n",
    "            base_name2 = dist2.replace(' Urban', '').replace(' Rural', '').replace(' District', '').strip()\n",
    "            \n",
    "            # If base names are identical, these might be variations to review\n",
    "            if base_name1 == base_name2 and dist1 != dist2:\n",
    "                count1 = state_district_counts[(state_district_counts['state'] == state) & \n",
    "                                               (state_district_counts['district'] == dist1)]['record_count'].values[0]\n",
    "                count2 = state_district_counts[(state_district_counts['state'] == state) & \n",
    "                                               (state_district_counts['district'] == dist2)]['record_count'].values[0]\n",
    "                duplicate_candidates.append({\n",
    "                    'state': state,\n",
    "                    'district_1': dist1,\n",
    "                    'count_1': count1,\n",
    "                    'district_2': dist2,\n",
    "                    'count_2': count2,\n",
    "                    'note': 'Possible variation (Urban/Rural/District suffix)'\n",
    "                })\n",
    "\n",
    "if duplicate_candidates:\n",
    "    print(f\"\\nFound {len(duplicate_candidates)} potential duplicate pairs:\\n\")\n",
    "    dup_df = pd.DataFrame(duplicate_candidates)\n",
    "    print(dup_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nNo obvious duplicate patterns detected (no same base names with different suffixes)\")\n",
    "\n",
    "# Display the full state-district summary\n",
    "print(\"\\n=== Complete State-District Summary ===\")\n",
    "print(state_district_counts.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5e37db",
   "metadata": {},
   "source": [
    "## Section 6: Review and Drop Rows with Missing Districts\n",
    "\n",
    "Display rows with missing districts, then remove them from the cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3593efee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing districts before dropping\n",
    "missing_districts_mask = df_cleaned['district'].isna()\n",
    "missing_count = missing_districts_mask.sum()\n",
    "\n",
    "print(\"=== Missing Districts Review ===\")\n",
    "print(f\"Total rows with missing districts: {missing_count}\")\n",
    "\n",
    "if missing_count > 0:\n",
    "    print(\"\\nSample of rows with missing districts:\")\n",
    "    print(df_cleaned[missing_districts_mask].head(10))\n",
    "    \n",
    "    print(\"\\nMissing districts by state:\")\n",
    "    print(df_cleaned[missing_districts_mask]['state'].value_counts())\n",
    "\n",
    "# Drop rows where district is NaN after all cleaning steps\n",
    "print(f\"\\n=== Dropping Missing Districts ===\")\n",
    "print(f\"Rows before dropping: {len(df_cleaned)}\")\n",
    "df_cleaned = df_cleaned[~missing_districts_mask].reset_index(drop=True)\n",
    "print(f\"Rows after dropping: {len(df_cleaned)}\")\n",
    "print(f\"Rows removed: {missing_count}\")\n",
    "\n",
    "# Verify no missing districts remain\n",
    "remaining_missing = df_cleaned['district'].isna().sum()\n",
    "print(f\"\\nRemaining missing districts: {remaining_missing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27d94cd",
   "metadata": {},
   "source": [
    "## Section 7: Export Cleaned Data and Generate Report\n",
    "\n",
    "Save the cleaned dataframe and create a comprehensive report of the cleaning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779bf100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataframe\n",
    "output_file = os.path.join(data_dir, 'aadhar_enrolment_cleaned.csv')\n",
    "df_cleaned.to_csv(output_file, index=False)\n",
    "print(f\"âœ“ Cleaned dataset saved to: {output_file}\")\n",
    "\n",
    "# Generate cleaning report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLEANING REPORT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nðŸ“Š Dataset Overview:\")\n",
    "print(f\"  â€¢ Original records: {len(df)}\")\n",
    "print(f\"  â€¢ Final records: {len(df_cleaned)}\")\n",
    "print(f\"  â€¢ Records removed: {len(df) - len(df_cleaned)}\")\n",
    "\n",
    "print(\"\\nðŸ”„ Standardization Applied:\")\n",
    "print(f\"  â€¢ Invalid values replaced: {mask.sum()}\")\n",
    "print(f\"  â€¢ All districts: Title cased, spaces normalized\")\n",
    "\n",
    "print(\"\\nðŸ“‹ District-State Pairs:\")\n",
    "print(f\"  â€¢ Total unique (state, district) pairs: {df_cleaned.groupby(['state', 'district']).size().shape[0]}\")\n",
    "print(f\"  â€¢ States represented: {df_cleaned['state'].nunique()}\")\n",
    "\n",
    "print(\"\\nâœ… Data Quality Verification:\")\n",
    "print(f\"  â€¢ Missing districts remaining: {df_cleaned['district'].isna().sum()}\")\n",
    "print(f\"  â€¢ Missing states remaining: {df_cleaned['state'].isna().sum()}\")\n",
    "\n",
    "# Show data types\n",
    "print(\"\\nðŸ“Œ Final Data Types:\")\n",
    "print(df_cleaned.dtypes)\n",
    "\n",
    "print(\"\\nâœ“ Sample of cleaned data:\")\n",
    "print(df_cleaned.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0378dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export state-district summary for reference\n",
    "state_district_summary = df_cleaned.groupby(['state', 'district']).agg({\n",
    "    'date': 'count',\n",
    "    'age_0_5': ['min', 'max', 'mean'],\n",
    "    'age_5_17': ['min', 'max', 'mean'],\n",
    "    'age_18_greater': ['min', 'max', 'mean']\n",
    "}).round(2)\n",
    "\n",
    "state_district_summary.columns = ['record_count', 'age_0_5_min', 'age_0_5_max', 'age_0_5_mean',\n",
    "                                   'age_5_17_min', 'age_5_17_max', 'age_5_17_mean',\n",
    "                                   'age_18_greater_min', 'age_18_greater_max', 'age_18_greater_mean']\n",
    "\n",
    "state_district_summary = state_district_summary.reset_index().sort_values(['state', 'district'])\n",
    "\n",
    "summary_file = os.path.join(data_dir, 'state_district_summary.csv')\n",
    "state_district_summary.to_csv(summary_file, index=False)\n",
    "print(f\"âœ“ State-District summary saved to: {summary_file}\")\n",
    "\n",
    "# Export duplicate candidates for manual review (if any)\n",
    "if duplicate_candidates:\n",
    "    dup_df.to_csv(os.path.join(data_dir, 'potential_duplicates_for_review.csv'), index=False)\n",
    "    print(f\"âœ“ Potential duplicates saved for manual review\")\n",
    "else:\n",
    "    print(f\"âœ“ No obvious duplicates detected\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLEANING COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nOutput Files Generated:\")\n",
    "print(f\"  1. {output_file}\")\n",
    "print(f\"  2. {summary_file}\")\n",
    "if duplicate_candidates:\n",
    "    print(f\"  3. {os.path.join(data_dir, 'potential_duplicates_for_review.csv')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
